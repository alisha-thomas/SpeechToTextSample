{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from jiwer import wer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lists_from_files(audio_file_path='audio_files.txt', transcript_file_path='transcriptions.txt'):\n",
    "    with open(audio_file_path, 'r') as af:\n",
    "        audio_files = [line.strip() for line in af]\n",
    "\n",
    "    with open(transcript_file_path, 'r') as tf:\n",
    "        transcriptions = [line.strip() for line in tf]\n",
    "\n",
    "    return audio_files, transcriptions\n",
    "\n",
    "# Load the lists back from text files\n",
    "audio_files, transcriptions = load_lists_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27}\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level vocabulary\n",
    "vocab = set(''.join(transcriptions))\n",
    "vocab = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "idx_to_char = {idx: char for char, idx in vocab.items()}\n",
    "blank_token_idx = len(vocab)\n",
    "print(vocab)\n",
    "\n",
    "# Function to convert transcription to numerical labels\n",
    "def text_to_labels(text):\n",
    "    return [vocab[char] for char in text]\n",
    "\n",
    "# Function to convert labels to text\n",
    "def labels_to_text(labels):\n",
    "    return ''.join([idx_to_char[idx] for idx in labels if idx in idx_to_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeechDataset class definition\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, audio_files, transcriptions, sample_rate=16000):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcriptions = transcriptions\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resample = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        self.melspec = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {self.audio_files[idx]}: {e}\")\n",
    "            return None, None  # Return None for both to handle it later\n",
    "        \n",
    "        waveform = self.resample(waveform)\n",
    "        mel_spec = self.melspec(waveform)\n",
    "        transcription = self.transcriptions[idx]\n",
    "        return mel_spec.squeeze(0).transpose(0, 1), transcription  # Transpose to [seq_len, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad sequences\n",
    "def pad_sequence(batch):\n",
    "    batch = [item for item in batch]  # Ensure [seq_len, feature_dim]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the collate function to handle None entries\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]  # Filter out None entries\n",
    "    if len(batch) == 0:  # Handle the case where all items are None\n",
    "        return None, None, None, None\n",
    "    mel_specs = [item[0] for item in batch]\n",
    "    transcriptions = [item[1] for item in batch]\n",
    "    mel_specs_padded = pad_sequence(mel_specs)\n",
    "    labels = [torch.tensor(text_to_labels(t)) for t in transcriptions]\n",
    "    label_lengths = torch.tensor([len(label) for label in labels])\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    input_lengths = torch.tensor([mel_spec.size(0) for mel_spec in mel_specs_padded])\n",
    "    return mel_specs_padded, labels_padded, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Alisha\\Alisha_hand_gesture\\speechtotext\\.venv\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader with the expanded dataset\n",
    "dataset = SpeechDataset(audio_files, transcriptions)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model with Convolutional Layer\n",
    "class EnhancedSpeechModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EnhancedSpeechModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(64, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Change to [batch_size, feature_dim, seq_len] for Conv1d\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.transpose(1, 2)  # Change back to [batch_size, seq_len, feature_dim] for LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "input_dim = 128  # Number of mel bands\n",
    "hidden_dim = 256\n",
    "output_dim = len(vocab) + 1  # Output dimension based on the size of the vocabulary + 1 for the blank token\n",
    "model = EnhancedSpeechModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Early Stopping and Learning Rate Scheduler\n",
    "def train_model(model, dataloader, num_epochs=100, learning_rate=0.001, patience=10):\n",
    "    criterion = nn.CTCLoss(blank=output_dim - 1, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            if batch[0] is None:  # Skip if batch is None\n",
    "                continue\n",
    "            \n",
    "            mel_specs, labels, input_lengths, label_lengths = batch\n",
    "            \n",
    "            if mel_specs.dim() != 3:\n",
    "                print(f\"Unexpected dimensions: {mel_specs.shape}\")\n",
    "                continue  # Skip this batch if dimensions are not as expected\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            outputs = outputs.permute(1, 0, 2)  # (T, N, C) for CTCLoss\n",
    "\n",
    "            loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(dataloader)\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Alisha\\Alisha_hand_gesture\\speechtotext\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.8048\n",
      "Epoch [2/100], Loss: 2.9166\n",
      "Epoch [3/100], Loss: 2.8721\n",
      "Epoch [4/100], Loss: 2.8465\n",
      "Epoch [5/100], Loss: 2.7416\n",
      "Epoch [6/100], Loss: 2.6708\n",
      "Epoch [7/100], Loss: 2.6013\n",
      "Epoch [8/100], Loss: 2.5149\n",
      "Epoch [9/100], Loss: 2.4340\n",
      "Epoch [10/100], Loss: 2.3511\n",
      "Epoch [11/100], Loss: 2.2715\n",
      "Epoch [12/100], Loss: 2.2021\n",
      "Epoch [13/100], Loss: 2.1319\n",
      "Epoch [14/100], Loss: 2.0649\n",
      "Epoch [15/100], Loss: 1.9990\n",
      "Epoch [16/100], Loss: 1.9470\n",
      "Epoch [17/100], Loss: 1.8937\n",
      "Epoch [18/100], Loss: 1.8295\n",
      "Epoch [19/100], Loss: 1.7699\n",
      "Epoch [20/100], Loss: 1.7199\n",
      "Epoch [21/100], Loss: 1.6685\n",
      "Epoch [22/100], Loss: 1.6139\n",
      "Epoch [23/100], Loss: 1.5633\n",
      "Epoch [24/100], Loss: 1.5075\n",
      "Epoch [25/100], Loss: 1.4565\n",
      "Epoch [26/100], Loss: 1.4189\n",
      "Epoch [27/100], Loss: 1.3504\n",
      "Epoch [28/100], Loss: 1.3013\n",
      "Epoch [29/100], Loss: 1.2417\n",
      "Epoch [30/100], Loss: 1.1895\n",
      "Epoch [31/100], Loss: 1.1455\n",
      "Epoch [32/100], Loss: 1.0921\n",
      "Epoch [33/100], Loss: 1.0389\n",
      "Epoch [34/100], Loss: 0.9957\n",
      "Epoch [35/100], Loss: 0.9460\n",
      "Epoch [36/100], Loss: 0.9103\n",
      "Epoch [37/100], Loss: 0.8608\n",
      "Epoch [38/100], Loss: 0.8117\n",
      "Epoch [39/100], Loss: 0.7715\n",
      "Epoch [40/100], Loss: 0.7326\n",
      "Epoch [41/100], Loss: 0.6865\n",
      "Epoch [42/100], Loss: 0.6485\n",
      "Epoch [43/100], Loss: 0.6109\n",
      "Epoch [44/100], Loss: 0.6019\n",
      "Epoch [45/100], Loss: 0.5427\n",
      "Epoch [46/100], Loss: 0.5065\n",
      "Epoch [47/100], Loss: 0.4826\n",
      "Epoch [48/100], Loss: 0.4632\n",
      "Epoch [49/100], Loss: 0.4535\n",
      "Epoch [50/100], Loss: 0.4287\n",
      "Epoch [51/100], Loss: 0.3865\n",
      "Epoch [52/100], Loss: 0.3547\n",
      "Epoch [53/100], Loss: 0.3347\n",
      "Epoch [54/100], Loss: 0.3139\n",
      "Epoch [55/100], Loss: 0.3055\n",
      "Epoch [56/100], Loss: 0.3023\n",
      "Epoch [57/100], Loss: 0.2753\n",
      "Epoch [58/100], Loss: 0.2524\n",
      "Epoch [59/100], Loss: 0.2405\n",
      "Epoch [60/100], Loss: 0.2269\n",
      "Epoch [61/100], Loss: 0.2235\n",
      "Epoch [62/100], Loss: 0.2058\n",
      "Epoch [63/100], Loss: 0.2128\n",
      "Epoch [64/100], Loss: 0.2314\n",
      "Epoch [65/100], Loss: 0.3011\n",
      "Epoch [66/100], Loss: 0.2478\n",
      "Epoch [67/100], Loss: 0.1601\n",
      "Epoch [68/100], Loss: 0.1220\n",
      "Epoch [69/100], Loss: 0.0990\n",
      "Epoch [70/100], Loss: 0.0904\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.1219\n",
      "Epoch [73/100], Loss: 0.3067\n",
      "Epoch [74/100], Loss: 0.3359\n",
      "Epoch [75/100], Loss: 0.1888\n",
      "Epoch [76/100], Loss: 0.1147\n",
      "Epoch [77/100], Loss: 0.0701\n",
      "Epoch [78/100], Loss: 0.0511\n",
      "Epoch [79/100], Loss: 0.0439\n",
      "Epoch [80/100], Loss: 0.0394\n",
      "Epoch [81/100], Loss: 0.0365\n",
      "Epoch [82/100], Loss: 0.0338\n",
      "Epoch [83/100], Loss: 0.0319\n",
      "Epoch [84/100], Loss: 0.0313\n",
      "Epoch [85/100], Loss: 0.0284\n",
      "Epoch [86/100], Loss: 0.0282\n",
      "Epoch [87/100], Loss: 0.0288\n",
      "Epoch [88/100], Loss: 0.0361\n",
      "Epoch [89/100], Loss: 0.0766\n",
      "Epoch [90/100], Loss: 0.1048\n",
      "Epoch [91/100], Loss: 0.0548\n",
      "Epoch [92/100], Loss: 0.0340\n",
      "Epoch [93/100], Loss: 0.0242\n",
      "Epoch [94/100], Loss: 0.0201\n",
      "Epoch [95/100], Loss: 0.0180\n",
      "Epoch [96/100], Loss: 0.0168\n",
      "Epoch [97/100], Loss: 0.0159\n",
      "Epoch [98/100], Loss: 0.0153\n",
      "Epoch [99/100], Loss: 0.0146\n",
      "Epoch [100/100], Loss: 0.0137\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the expanded dataset\n",
    "train_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode the model output into text\n",
    "def decode_output(output):\n",
    "    _, max_indices = torch.max(output, dim=-1)\n",
    "    tokens = max_indices.unique_consecutive()\n",
    "    decoded = ''.join([idx_to_char[idx.item()] for idx in tokens if idx.item() in idx_to_char])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy using Word Error Rate (WER)\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch[0] is None:  # Skip if batch is None\n",
    "                continue\n",
    "            \n",
    "            mel_specs, labels, input_lengths, label_lengths = batch\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            outputs = outputs.permute(1, 0, 2)  # (T, N, C) for CTCLoss\n",
    "            \n",
    "            for i in range(outputs.size(1)):  # Iterate over batch\n",
    "                decoded_output = decode_output(outputs[:, i, :])\n",
    "                predictions.append(decoded_output)\n",
    "                ground_truths.append(''.join([idx_to_char[idx.item()] for idx in labels[i] if idx.item() in idx_to_char]))\n",
    "    \n",
    "    wer_score = wer(ground_truths, predictions)\n",
    "    accuracy = 1 - wer_score\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8158\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(model, dataloader)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model, 'speechtotextmodel3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedSpeechModel(\n",
       "  (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (lstm): LSTM(64, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_path = 'speechtotextmodel3.pth'  # Adjust this path as necessary\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to preprocess audio\n",
    "def preprocess_audio(file_path, sample_rate=16000):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    resample = Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "    waveform = resample(waveform)\n",
    "    melspec = MelSpectrogram(sample_rate=sample_rate, n_mels=128)\n",
    "    mel_spec = melspec(waveform)\n",
    "    return mel_spec.squeeze(0).transpose(0, 1)  # Transpose to [seq_len, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode the output\n",
    "def labels_to_text1(labels, idx_to_char1):\n",
    "    return ''.join([idx_to_char1[idx] for idx in labels if idx in idx_to_char1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab1 = {char: idx for idx, char in enumerate(\" 'ABCDEFGHIJKLMNOPQRSTUVWXYZ\")}\n",
    "idx_to_char1 = {idx: char for char, idx in vocab1.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the uploaded audio files\n",
    "audio_files = [\"31-121972-0000.wav\", \"sample2.wav\"]  # Adjust paths as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for 31-121972-0000.wav: WWWHHHELLHIFFO  WWO  ONLAUWWMONE DUTOM LED   ONN  OOOLOKKGREEAREZZ ONN EYY   T  ASTISSSIIEFOMMSHOYIT TLADD DI EF   OO  HHOE  I  DOCEEDU CEE   MAFTOTTT  SOOMMTOOO ONTTEREDSEETT   OOL  AOAAA  Y  MWOON   SWIO MMA I  HAASSLIIYYLLLY  ANRSUED DIGG MNTANNN\n",
      "Transcription for sample2.wav: BUTT SSHHHE   HADD NNOT  LOOKED RROUNND   LONNG  BBEEFFORE SHHE SAW HHIM  LLEEADDINGG A YOUNNG  LADY  TOOO THE  DAANCCE  AH   HEEEE    HASS  GOTT A PPARRTTNNER  I  WISHHH HEE   HAAD ASKKEED  YOOU  SAIDD  MISSSSSUS  ALLLEN AND  AFTEERR  A SHORTT   SILLLLLENNCE  SSHEE ADDDEDD\n"
     ]
    }
   ],
   "source": [
    "# Process and predict for each audio file\n",
    "transcriptions = []\n",
    "for audio_file in audio_files:\n",
    "    mel_spec = preprocess_audio(audio_file)\n",
    "    mel_spec = mel_spec.unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(mel_spec)\n",
    "    predicted_labels = torch.argmax(output, dim=-1).squeeze().tolist()\n",
    "    transcription = labels_to_text(predicted_labels)\n",
    "    transcriptions.append(transcription)\n",
    "    print(f\"Transcription for {audio_file}: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IN SSSPITE OF  UUDDDOLLLPHOOO  ANND THE DRRESSMMAKERR HHOOOWWEVERRR TTHE PARTTTYY FFROOM PPULTTENEY  SSSSTRREETTT  RREACHED THEEE    UPPPER RROOOMMSS  INN VVERRRY GOODD TTIIME  THE THOORRPES ANDD  JAMESS  MORLAANNNND WERE  THEERE  ONLLY TWO  MINUTES  BBBEEFORRE THEMM',\n",
       " 'BUTT SSHHHE   HADD NNOT  LOOKED RROUNND   LONNG  BBEEFFORE SHHE SAW HHIM  LLEEADDINGG A YOUNNG  LADY  TOOO THE  DAANCCE  AH   HEEEE    HASS  GOTT A PPARRTTNNER  I  WISHHH HEE   HAAD ASKKEED  YOOU  SAIDD  MISSSSSUS  ALLLEN AND  AFTEERR  A SHORTT   SILLLLLENNCE  SSHEE ADDDEDD']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
