{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lists_from_files(audio_file_path='audio_files.txt', transcript_file_path='transcriptions.txt'):\n",
    "    with open(audio_file_path, 'r') as af:\n",
    "        audio_files = [line.strip() for line in af]\n",
    "\n",
    "    with open(transcript_file_path, 'r') as tf:\n",
    "        transcriptions = [line.strip() for line in tf]\n",
    "\n",
    "    return audio_files, transcriptions\n",
    "\n",
    "# Load the lists back from text files\n",
    "audio_files, transcriptions = load_lists_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27}\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level vocabulary\n",
    "vocab = set(''.join(transcriptions))\n",
    "vocab = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "idx_to_char = {idx: char for char, idx in vocab.items()}\n",
    "blank_token_idx = len(vocab)\n",
    "print(vocab)\n",
    "\n",
    "# Function to convert transcription to numerical labels\n",
    "def text_to_labels(text):\n",
    "    return [vocab[char] for char in text]\n",
    "\n",
    "# Function to convert labels to text\n",
    "def labels_to_text(labels):\n",
    "    return ''.join([idx_to_char[idx] for idx in labels if idx in idx_to_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeechDataset class definition\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, audio_files, transcriptions, sample_rate=16000):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcriptions = transcriptions\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resample = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        self.melspec = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {self.audio_files[idx]}: {e}\")\n",
    "            return None, None  # Return None for both to handle it later\n",
    "        \n",
    "        waveform = self.resample(waveform)\n",
    "        mel_spec = self.melspec(waveform)\n",
    "        transcription = self.transcriptions[idx]\n",
    "        return mel_spec.squeeze(0).transpose(0, 1), transcription  # Transpose to [seq_len, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad sequences\n",
    "def pad_sequence(batch):\n",
    "    batch = [item for item in batch]  # Ensure [seq_len, feature_dim]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the collate function to handle None entries\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]  # Filter out None entries\n",
    "    if len(batch) == 0:  # Handle the case where all items are None\n",
    "        return None, None, None, None\n",
    "    mel_specs = [item[0] for item in batch]\n",
    "    transcriptions = [item[1] for item in batch]\n",
    "    mel_specs_padded = pad_sequence(mel_specs)\n",
    "    labels = [torch.tensor(text_to_labels(t)) for t in transcriptions]\n",
    "    label_lengths = torch.tensor([len(label) for label in labels])\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    input_lengths = torch.tensor([mel_spec.size(0) for mel_spec in mel_specs_padded])\n",
    "    return mel_specs_padded, labels_padded, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Alisha\\Alisha_hand_gesture\\speechtotext\\.venv\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader with the expanded dataset\n",
    "dataset = SpeechDataset(audio_files, transcriptions)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model architecture\n",
    "class SimpleSpeechModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleSpeechModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "input_dim = 128  # Number of mel bands\n",
    "hidden_dim = 256\n",
    "output_dim = len(vocab) + 1  # Output dimension based on the size of the vocabulary + 1 for the blank token\n",
    "model = SimpleSpeechModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CTCLoss(blank=output_dim - 1, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            if batch[0] is None:  # Skip if batch is None\n",
    "                continue\n",
    "            \n",
    "            mel_specs, labels, input_lengths, label_lengths = batch\n",
    "            \n",
    "            if mel_specs.dim() != 3:\n",
    "                print(f\"Unexpected dimensions: {mel_specs.shape}\")\n",
    "                continue  # Skip this batch if dimensions are not as expected\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            outputs = outputs.permute(1, 0, 2)  # (T, N, C) for CTCLoss\n",
    "\n",
    "            loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.4631\n",
      "Epoch [2/100], Loss: 2.9693\n",
      "Epoch [3/100], Loss: 2.9224\n",
      "Epoch [4/100], Loss: 2.9113\n",
      "Epoch [5/100], Loss: 2.8445\n",
      "Epoch [6/100], Loss: 2.8095\n",
      "Epoch [7/100], Loss: 2.7201\n",
      "Epoch [8/100], Loss: 2.6751\n",
      "Epoch [9/100], Loss: 2.6394\n",
      "Epoch [10/100], Loss: 2.6314\n",
      "Epoch [11/100], Loss: 2.6377\n",
      "Epoch [12/100], Loss: 2.6178\n",
      "Epoch [13/100], Loss: 2.5222\n",
      "Epoch [14/100], Loss: 2.4607\n",
      "Epoch [15/100], Loss: 2.3946\n",
      "Epoch [16/100], Loss: 2.3194\n",
      "Epoch [17/100], Loss: 2.5675\n",
      "Epoch [18/100], Loss: 2.5583\n",
      "Epoch [19/100], Loss: 2.4490\n",
      "Epoch [20/100], Loss: 2.4033\n",
      "Epoch [21/100], Loss: 2.3128\n",
      "Epoch [22/100], Loss: 2.2359\n",
      "Epoch [23/100], Loss: 2.1699\n",
      "Epoch [24/100], Loss: 2.1086\n",
      "Epoch [25/100], Loss: 2.0504\n",
      "Epoch [26/100], Loss: 1.9871\n",
      "Epoch [27/100], Loss: 1.9298\n",
      "Epoch [28/100], Loss: 1.8696\n",
      "Epoch [29/100], Loss: 1.8109\n",
      "Epoch [30/100], Loss: 1.7642\n",
      "Epoch [31/100], Loss: 1.7046\n",
      "Epoch [32/100], Loss: 1.6584\n",
      "Epoch [33/100], Loss: 1.6056\n",
      "Epoch [34/100], Loss: 1.5487\n",
      "Epoch [35/100], Loss: 1.4974\n",
      "Epoch [36/100], Loss: 1.4464\n",
      "Epoch [37/100], Loss: 1.4069\n",
      "Epoch [38/100], Loss: 1.3711\n",
      "Epoch [39/100], Loss: 1.3114\n",
      "Epoch [40/100], Loss: 1.2619\n",
      "Epoch [41/100], Loss: 1.2204\n",
      "Epoch [42/100], Loss: 1.1794\n",
      "Epoch [43/100], Loss: 1.1544\n",
      "Epoch [44/100], Loss: 1.1142\n",
      "Epoch [45/100], Loss: 1.0802\n",
      "Epoch [46/100], Loss: 1.0285\n",
      "Epoch [47/100], Loss: 0.9999\n",
      "Epoch [48/100], Loss: 0.9612\n",
      "Epoch [49/100], Loss: 0.9424\n",
      "Epoch [50/100], Loss: 0.8970\n",
      "Epoch [51/100], Loss: 0.8683\n",
      "Epoch [52/100], Loss: 0.8385\n",
      "Epoch [53/100], Loss: 0.8277\n",
      "Epoch [54/100], Loss: 0.8047\n",
      "Epoch [55/100], Loss: 0.7661\n",
      "Epoch [56/100], Loss: 0.7582\n",
      "Epoch [57/100], Loss: 0.7163\n",
      "Epoch [58/100], Loss: 0.6940\n",
      "Epoch [59/100], Loss: 0.6742\n",
      "Epoch [60/100], Loss: 0.6485\n",
      "Epoch [61/100], Loss: 0.6328\n",
      "Epoch [62/100], Loss: 0.6195\n",
      "Epoch [63/100], Loss: 0.6070\n",
      "Epoch [64/100], Loss: 0.5835\n",
      "Epoch [65/100], Loss: 0.6035\n",
      "Epoch [66/100], Loss: 0.5780\n",
      "Epoch [67/100], Loss: 0.5213\n",
      "Epoch [68/100], Loss: 0.5027\n",
      "Epoch [69/100], Loss: 0.4875\n",
      "Epoch [70/100], Loss: 0.4707\n",
      "Epoch [71/100], Loss: 0.4552\n",
      "Epoch [72/100], Loss: 0.4348\n",
      "Epoch [73/100], Loss: 0.4224\n",
      "Epoch [74/100], Loss: 0.4399\n",
      "Epoch [75/100], Loss: 0.4354\n",
      "Epoch [76/100], Loss: 0.4065\n",
      "Epoch [77/100], Loss: 0.3924\n",
      "Epoch [78/100], Loss: 0.3716\n",
      "Epoch [79/100], Loss: 0.3787\n",
      "Epoch [80/100], Loss: 0.3647\n",
      "Epoch [81/100], Loss: 0.3338\n",
      "Epoch [82/100], Loss: 0.3393\n",
      "Epoch [83/100], Loss: 0.3215\n",
      "Epoch [84/100], Loss: 0.3328\n",
      "Epoch [85/100], Loss: 0.3320\n",
      "Epoch [86/100], Loss: 0.3180\n",
      "Epoch [87/100], Loss: 0.3041\n",
      "Epoch [88/100], Loss: 0.3051\n",
      "Epoch [89/100], Loss: 0.2916\n",
      "Epoch [90/100], Loss: 0.2615\n",
      "Epoch [91/100], Loss: 0.2545\n",
      "Epoch [92/100], Loss: 0.2590\n",
      "Epoch [93/100], Loss: 0.2714\n",
      "Epoch [94/100], Loss: 0.2543\n",
      "Epoch [95/100], Loss: 0.2575\n",
      "Epoch [96/100], Loss: 0.2403\n",
      "Epoch [97/100], Loss: 0.2295\n",
      "Epoch [98/100], Loss: 0.2147\n",
      "Epoch [99/100], Loss: 0.2367\n",
      "Epoch [100/100], Loss: 0.2907\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the expanded dataset\n",
    "train_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode the model output into text\n",
    "def decode_output(output):\n",
    "    _, max_indices = torch.max(output, dim=-1)\n",
    "    tokens = max_indices.unique_consecutive()\n",
    "    decoded = ''.join([idx_to_char[idx.item()] for idx in tokens if idx.item() in idx_to_char])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy using Word Error Rate (WER)\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch[0] is None:  # Skip if batch is None\n",
    "                continue\n",
    "            \n",
    "            mel_specs, labels, input_lengths, label_lengths = batch\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            outputs = outputs.permute(1, 0, 2)  # (T, N, C) for CTCLoss\n",
    "            \n",
    "            for i in range(outputs.size(1)):  # Iterate over batch\n",
    "                decoded_output = decode_output(outputs[:, i, :])\n",
    "                predictions.append(decoded_output)\n",
    "                ground_truths.append(''.join([idx_to_char[idx.item()] for idx in labels[i] if idx.item() in idx_to_char]))\n",
    "    \n",
    "    wer_score = wer(ground_truths, predictions)\n",
    "    accuracy = 1 - wer_score\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.6648\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(model, dataloader)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
