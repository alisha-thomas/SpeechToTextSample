{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the uploaded audio files\n",
    "audio_files = [\"sample1.wav\", \"sample2.wav\"]  # List of audio file paths\n",
    "transcriptions = [\"IN SPITE OF UDOLPHO AND THE DRESSMAKER HOWEVER THE PARTY FROM PULTENEY STREET REACHED THE UPPER ROOMS IN VERY GOOD TIME THE THORPES AND JAMES MORLAND WERE THERE ONLY TWO MINUTES BEFORE THEM\", \n",
    "                  \"BUT SHE HAD NOT LOOKED ROUND LONG BEFORE SHE SAW HIM LEADING A YOUNG LADY TO THE DANCE AH HE HAS GOT A PARTNER I WISH HE HAD ASKED YOU SAID MISSUS ALLEN AND AFTER A SHORT SILENCE SHE ADDED\"]  # Corresponding transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, audio_files, transcriptions, sample_rate=16000):\n",
    "        self.audio_files = audio_files\n",
    "        self.transcriptions = transcriptions\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resample = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        self.melspec = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n",
    "        waveform = self.resample(waveform)\n",
    "        mel_spec = self.melspec(waveform)\n",
    "        transcription = self.transcriptions[idx]\n",
    "        return mel_spec.squeeze(0).transpose(0, 1), transcription  # Transpose to [seq_len, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_sequence(batch):\n",
    "#     batch = [item.transpose(0, 1) for item in batch]\n",
    "#     batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "#     batch = batch.transpose(1, 2)  # Restore original dimensions\n",
    "#     return batch\n",
    "\n",
    "# def pad_sequence(batch):\n",
    "#     batch = [item[0] for item in batch]\n",
    "#     batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "#     return batch\n",
    "\n",
    "# def pad_sequence(batch):\n",
    "#     batch = [item[0].transpose(0, 1) for item in batch]\n",
    "#     batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "#     return batch\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    batch = [item for item in batch]  # Ensure [seq_len, feature_dim]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     mel_specs = [item[0].squeeze(0) for item in batch]\n",
    "#     transcriptions = [item[1] for item in batch]\n",
    "#     mel_specs_padded = pad_sequence(mel_specs)\n",
    "#     return mel_specs_padded, transcriptions\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     mel_specs = [item[0] for item in batch]\n",
    "#     transcriptions = [item[1] for item in batch]\n",
    "#     mel_specs_padded = pad_sequence(mel_specs)\n",
    "#     return mel_specs_padded, transcriptions\n",
    "\n",
    "def collate_fn(batch):\n",
    "    mel_specs = [item[0] for item in batch]\n",
    "    transcriptions = [item[1] for item in batch]\n",
    "    mel_specs_padded = pad_sequence(mel_specs)\n",
    "    return mel_specs_padded, transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model architecture\n",
    "class SimpleSpeechModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleSpeechModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "input_dim = 128  # Number of mel bands\n",
    "hidden_dim = 256\n",
    "output_dim = 10  # Example output dimension, adjust based on your transcription encoding\n",
    "model = SimpleSpeechModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for mel_specs, transcriptions in dataloader:\n",
    "            # Convert transcriptions to tensor of labels\n",
    "            # Note: In a real scenario, you'll need to preprocess and convert transcriptions to a suitable format (e.g., label encoding)\n",
    "            labels = torch.tensor([0, 1])  # Dummy labels for example purposes\n",
    "            \n",
    "            if mel_specs.dim() != 3:\n",
    "                print(f\"Unexpected dimensions: {mel_specs.shape}\")\n",
    "                continue  # Skip this batch if dimensions are not as expected\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Alisha\\Alisha_hand_gesture\\speechtotext\\.venv\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = SpeechDataset(audio_files, transcriptions)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2982\n",
      "Epoch [2/10], Loss: 2.2673\n",
      "Epoch [3/10], Loss: 2.2362\n",
      "Epoch [4/10], Loss: 2.2031\n",
      "Epoch [5/10], Loss: 2.1658\n",
      "Epoch [6/10], Loss: 2.1202\n",
      "Epoch [7/10], Loss: 2.0590\n",
      "Epoch [8/10], Loss: 1.9649\n",
      "Epoch [9/10], Loss: 1.7768\n",
      "Epoch [10/10], Loss: 1.0158\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
